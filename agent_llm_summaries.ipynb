{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken\n",
    "import pandas as pd \n",
    "# Set display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992eccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install tiktoken tenacity tqdm google-generativeai\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "import os\n",
    "from scipy.stats import linregress\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import tiktoken\n",
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205686ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('/Users/rohankajgaonkar/projects/agents/.env')\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tiktoken\n",
    "import google.generativeai as genai\n",
    "\n",
    "# =========================================================\n",
    "# Gemini configuration\n",
    "# =========================================================\n",
    "GEMINI_API_KEY = os.getenv('GOOGLE_API_KEY_2')\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "GEMINI_MODEL = \"models/gemini-2.5-flash\"\n",
    "\n",
    "# =========================================================\n",
    "# BigQuery client\n",
    "# =========================================================\n",
    "client = bigquery.Client(project=\"masked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec075ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# Processing Configuration\n",
    "# =========================================================\n",
    "MAX_WORKERS = min(32, multiprocessing.cpu_count() * 4)  # Dynamic scaling\n",
    "SAMPLE_SIZE = 10  # Change to desired sample size\n",
    "RANDOM_SEED = 12345\n",
    "\n",
    "print(f\"üîß Configuration loaded:\")\n",
    "print(f\"   - Max workers: {MAX_WORKERS}\")\n",
    "print(f\"   - Model: {GEMINI_MODEL}\")\n",
    "print(f\"   - Sample size: {SAMPLE_SIZE}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9404457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# Token counting (monitoring only)\n",
    "# =========================================================\n",
    "_enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text for cost estimation\"\"\"\n",
    "    return len(_enc.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# Gemini API with retry logic\n",
    "# =========================================================\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    reraise=True\n",
    ")\n",
    "def query_gemini(\n",
    "    prompt: str,\n",
    "    model_name: str = GEMINI_MODEL,\n",
    "    temperature: float = 0,\n",
    "    max_output_tokens: int = 10248,\n",
    "    timeout: int = 120\n",
    "):\n",
    "    \"\"\"Query Gemini with automatic retry on failure\"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name,\n",
    "        generation_config={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_output_tokens,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            request_options={\"timeout\": timeout}\n",
    "        )\n",
    "        \n",
    "        if not response or not response.text:\n",
    "            raise ValueError(\"Empty response from Gemini\")\n",
    "            \n",
    "        return response.text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gemini request failed: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Trend helpers\n",
    "# =========================================================\n",
    "def classify_slope(slope, y):\n",
    "    \"\"\"Classify numeric slope into qualitative trend\"\"\"\n",
    "    non_zero_count = (y != 0).sum()\n",
    "    unique_vals = y.dropna().unique()\n",
    "\n",
    "    if len(unique_vals) == 1 and unique_vals[0] == 0:\n",
    "        return \"no trend\"\n",
    "    if non_zero_count <= 1:\n",
    "        return \"no trend\"\n",
    "    if len(unique_vals) == 1:\n",
    "        return \"consistent\"\n",
    "    if slope < 0:\n",
    "        return \"downward trend\"\n",
    "    if slope > 0:\n",
    "        return \"upward trend\"\n",
    "    return \"consistent\"\n",
    "\n",
    "\n",
    "def compute_trend(df, col):\n",
    "    \"\"\"Compute linear regression trend for a metric over time\"\"\"\n",
    "    if col not in df.columns or \"month_index\" not in df.columns:\n",
    "        return 0.0, \"no trend\"\n",
    "\n",
    "    y = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    x = pd.to_numeric(df[\"month_index\"], errors=\"coerce\")\n",
    "\n",
    "    mask = y.notna()\n",
    "    x, y = x[mask], y[mask]\n",
    "\n",
    "    if len(y) < 2:\n",
    "        return 0.0, \"no trend\"\n",
    "\n",
    "    if y.nunique() == 1 and y.iloc[0] == 0:\n",
    "        return 0.0, \"no trend\"\n",
    "\n",
    "    slope, *_ = linregress(x.values, y.values)\n",
    "    return slope, classify_slope(slope, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb533edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# JSON serialization helper\n",
    "# =========================================================\n",
    "def serialize_for_json(obj):\n",
    "    \"\"\"Convert pandas/datetime objects to JSON-safe formats\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: serialize_for_json(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [serialize_for_json(v) for v in obj]\n",
    "    if hasattr(obj, \"isoformat\"):\n",
    "        try:\n",
    "            return obj.isoformat()\n",
    "        except Exception:\n",
    "            return str(obj)\n",
    "    return obj\n",
    "\n",
    "# Cell 8: Load Data Dictionary\n",
    "# =========================================================\n",
    "# Load data dictionary\n",
    "# =========================================================\n",
    "with open(\"/Users/rohankajgaonkar/Downloads/llm_data_dictionary.json\", \"r\") as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "print(f\"üìö Loaded data dictionary with {len(data_dict)} fields\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# Agent Archetypes\n",
    "# =========================================================\n",
    "archetypes = [\n",
    "    {\n",
    "        \"archetype\": \"Newbie\",\n",
    "        \"pain_points\": [\n",
    "            \"Struggle to win first client trust\",\n",
    "            \"Unsure which tools deliver results; risk of wasting spend\",\n",
    "            \"Easily overwhelmed by many features and options\",\n",
    "            \"Can get discouraged if results don't come quickly\"\n",
    "        ],\n",
    "        \"values\": [\n",
    "            \"Affordable, low-risk way to start\",\n",
    "            \"Simple, guided steps that show what to do next\",\n",
    "            \"Quick visibility and credibility signals (Verified Listings, reviews)\",\n",
    "            \"Early proof that actions lead to results\"\n",
    "        ],\n",
    "        \"PG_messaging_principles\": [\n",
    "            \"Position PG as a safe and guided entry point to start their career\",\n",
    "            \"Emphasise credibility and visibility from day one\",\n",
    "            \"Show tools as simple helpers that build confidence step by step\",\n",
    "            \"Reinforce early wins to boost momentum and confidence\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"archetype\": \"Grind-to-Breakthrough\",\n",
    "        \"pain_points\": [\n",
    "            \"Work hard but see uneven results; credits burn quickly\",\n",
    "            \"Lack scalable strategies; progress may feel slow\",\n",
    "            \"Pressure to prove themselves with bigger wins\"\n",
    "        ],\n",
    "        \"values\": [\n",
    "            \"Efficiency hacks to stretch credits and effort\",\n",
    "            \"Clear ROI proof to justify spend\",\n",
    "            \"Validation that they're on the right growth path\",\n",
    "            \"Tools that feel like next-level upgrades\"\n",
    "        ],\n",
    "        \"PG_messaging_principles\": [\n",
    "            \"Emphasise efficiency and ROI\",\n",
    "            \"Highlight tools that provide a competitive edge over peers\",\n",
    "            \"Reinforce that PG turns hard work into measurable progress\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"archetype\": \"Grind-to-Sustain\",\n",
    "        \"pain_points\": [\n",
    "            \"Risk of burnout from heavy workload\",\n",
    "            \"Time-poor; may be reluctant to adopt new tools\",\n",
    "            \"Seeks stability, not risky experiments\"\n",
    "        ],\n",
    "        \"values\": [\n",
    "            \"Reliable, repeatable lead flow\",\n",
    "            \"Tools that save time and reduce admin effort\",\n",
    "            \"Credibility signals that protect reputation\"\n",
    "        ],\n",
    "        \"PG_messaging_principles\": [\n",
    "            \"Emphasise time-saving and lead gen features\",\n",
    "            \"Show tools as reliable and easy to adopt into daily routines\",\n",
    "            \"Reinforce PG's role in protecting income and maintaining edge\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"archetype\": \"Driven Achiever\",\n",
    "        \"pain_points\": [\n",
    "            \"Highly competitive; want maximum visibility\",\n",
    "            \"Pressure to maintain growth and status\",\n",
    "            \"Need to scale across multiple listings or a team\"\n",
    "        ],\n",
    "        \"values\": [\n",
    "            \"Top visibility and reach across all channels\",\n",
    "            \"Recognition as industry leaders\",\n",
    "            \"Premium tools that signal professionalism and success\",\n",
    "            \"For team leads: tools that boost team efficiency and credibility\"\n",
    "        ],\n",
    "        \"PG_messaging_principles\": [\n",
    "            \"Emphasise prestige and maximum exposure with premium tools\",\n",
    "            \"Reinforce recognition and competitive advantage (awards, rankings, visibility)\",\n",
    "            \"For team leads: highlight team enablement and scaling productivity\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"archetype\": \"Industry Icon\",\n",
    "        \"pain_points\": [\n",
    "            \"Already credible; focus shifts to influence\",\n",
    "            \"Desire to stay relevant at the top\",\n",
    "            \"Need recognition beyond transactions (awards, prestige)\"\n",
    "        ],\n",
    "        \"values\": [\n",
    "            \"Public recognition and prestige at the highest level\",\n",
    "            \"Tools that showcase reputation and leadership\",\n",
    "            \"Platforms that amplify voice and influence\"\n",
    "        ],\n",
    "        \"PG_messaging_principles\": [\n",
    "            \"Emphasise prestige signals (awards, media, platform presence)\",\n",
    "            \"Reinforce PG as the platform where icons are recognised\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"archetype\": \"Part-Timer / Semi-Retired\",\n",
    "        \"pain_points\": [\n",
    "            \"Low or irregular activity; property work is secondary income or hobby\",\n",
    "            \"Feels disconnected from market trends and younger peers\",\n",
    "            \"Hesitant to invest time in learning complex features\"\n",
    "        ],\n",
    "        \"values\": [\n",
    "            \"Flexibility to use only when needed, without pressure\",\n",
    "            \"Credibility and trust signals that help maintain reputation despite low activity\",\n",
    "            \"Simple reminders rather than hard-sell messaging\"\n",
    "        ],\n",
    "        \"PG_messaging_principles\": [\n",
    "            \"Position PG as the easy, low-effort partner that keeps them credible and current\",\n",
    "            \"Emphasise simplicity, flexibility, and time-saving convenience\",\n",
    "            \"Use simple and low-pressure communication with no complexity or pushy messages\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\" Loaded {len(archetypes)} agent archetypes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c70074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48847529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Query BigQuery for Agent Data For Renewals\n",
    "# =========================================================\n",
    "\n",
    "main_query = f\"\"\"\n",
    "WITH agents_renewing_in_feb AS (\n",
    "    -- First, find the unique agent_ids whose subscription ends in Feb 2026\n",
    "    SELECT DISTINCT agent_id\n",
    "    FROM `masked`\n",
    "    WHERE subscription_end >= '2026-02-01' AND subscription_end < '2026-03-01'\n",
    ")\n",
    "-- Then, fetch all monthly data for that specific cohort of agents\n",
    "SELECT T1.*\n",
    "FROM `masked` AS T1\n",
    "JOIN agents_renewing_in_feb AS T2 ON T1.agent_id = T2.agent_id\n",
    "ORDER BY T1.agent_id, T1.data_month\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì• Fetching data for the target agent cohort...\")\n",
    "agent_df_all = client.query(main_query).to_dataframe()\n",
    "\n",
    "if agent_df_all.empty:\n",
    "    print(\"No agents found with a subscription ending in February 2026. Please check the date range or data.\")\n",
    "else:\n",
    "    print(f\" Retrieved {len(agent_df_all):,} rows for {agent_df_all['agent_id'].nunique()} agents.\")\n",
    "    print(f\"Date range of fetched data: {agent_df_all['data_month'].min()} to {agent_df_all['data_month'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191295ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df_all['agent_id'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1571d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df_all[agent_df_all['agent_id'] == 624971]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc54483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = \"/Users/rohankajgaonkar/Downloads/llm_dictionary_l365.json\"\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "# List files in Downloads to see what's there\n",
    "downloads_path = \"/Users/rohankajgaonkar/Downloads\"\n",
    "json_files = [f for f in os.listdir(downloads_path) if f.endswith('.json')]\n",
    "print(f\"JSON files in Downloads: {json_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a634c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# file_path = \"/Users/rohankajgaonkar/Downloads/llm_dictionary_l365.json\"\n",
    "# print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "# # List files in Downloads to see what's there\n",
    "# downloads_path = \"/Users/rohankajgaonkar/Downloads\"\n",
    "# json_files = [f for f in os.listdir(downloads_path) if f.endswith('.json')]\n",
    "# print(f\"JSON files in Downloads: {json_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Filter for the specific agent (assuming from cell 11)\n",
    "# df = agent_df_all[agent_df_all['agent_id'] == 624971]\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(1,2,sharex=True, sharey=True, figsize=(12, 3))\n",
    "# # ax.plot(df['data_month'], df['activate_action'])\n",
    "# # ax.set_title('Activate Action Over Time for Agent 624971')\n",
    "# # ax.set_xlabel('Data Month')\n",
    "# # ax.set_ylabel('Activate Action')\n",
    "# ax1.plot(df['data_month'], df['activate_action'])\n",
    "# ax2.plot(df['data_month'], df['repost_action'])\n",
    "# ax1.set_title('Activate Action Over Time for Agent 624971')\n",
    "# ax1.set_xlabel('Data Month')\n",
    "# ax1.set_ylabel('Activate Action')\n",
    "# ax2.set_title('Repost Action Over Time for Agent 624971')\n",
    "# ax2.set_xlabel('Data Month')\n",
    "# ax2.set_ylabel('reposts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique agents and their projected status\n",
    "unique_agents_projected_status = agent_df_all.drop_duplicates(subset='agent_id')[['agent_id', 'firstname', 'lastname', 'projected_status_l365']]\n",
    "unique_agents_projected_status['projected_status_l365'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#  Prompt Builder \n",
    "# =========================================================\n",
    "def extract_relevant_fields(data_dict, agent_df):\n",
    "    \"\"\"Extract only field definitions that exist in agent data\"\"\"\n",
    "    agent_columns = set(agent_df.columns)\n",
    "    relevant_dict = {k: v for k, v in data_dict.items() if k in agent_columns}\n",
    "    return json.dumps(relevant_dict, indent=2)\n",
    "\n",
    "\n",
    "def build_prompt_for_single_agent(single_agent_df):\n",
    "    \"\"\"Build optimized prompt for single agent analysis\"\"\"\n",
    "    \n",
    "    # Subset columns\n",
    "    cols = [\n",
    "        'agent_id', 'data_month', 'firstname', 'lastname', 'age', 'active_days',\n",
    "        'subscription_start', 'subscription_end', 'current_subscription',\n",
    "        'credit_entitlement', 'credit_accumulation_cap', 'credit_purchase_cap',\n",
    "        'loyalty_duration', 'estimated_earnings', 'total_listings',\n",
    "        'sale_listings', 'rent_listings', 'roomrent_listings',\n",
    "        'new_launch_condo_listings', 'new_launch_excondo_listings',\n",
    "        'landed_sale_listings', 'landed_rent_listings', 'landed_roomrent_listings',\n",
    "        'hdb_sale_listings', 'hdb_rent_listings', 'hdb_roomrent_listings',\n",
    "        'condo_sale_listings', 'condo_rent_listings', 'condo_roomrent_listings',\n",
    "        'com_sale_listings', 'com_rent_listings', 'listing_segment',\n",
    "        'sub_ad_credit_consumed', 'disc_ad_credit_consumed',\n",
    "        'landed_sale_transactions', 'hdb_sale_transactions',\n",
    "        'condo_sale_transactions', 'landed_rent_transactions',\n",
    "        'hdb_rent_transactions', 'condo_rent_transactions', 'leads', 'impressions',\n",
    "        'monthly_sub_spend', 'monthly_disc_ac_spend', 'monthly_disc_pc_spend',\n",
    "        'boost_incl_repost_action', 'turbo_incl_repost_action',\n",
    "        'promote_list_action', 'leads_per_listing', 'impressions_per_listing',\n",
    "        'archetype',\n",
    "        'l365_total_listings', 'l365_ad_credit_consumed',\n",
    "        'projected_tier_l365_ad_credit_consumed',\n",
    "        'projected_tier_l365_com_listings',\n",
    "        'projected_tier_l365_newlaunch_listings', 'l365_spend',\n",
    "        'projected_tier_l365_spend_earnings_decile',\n",
    "        'projected_subscription_l365', 'projected_status_l365',\n",
    "        'l365_ad_credit_consumed_tier_decile', 'l365_spend_tier_decile',\n",
    "        'projected_early_upgrade_potential_l365',\n",
    "        'l30_total_listings', 'projected_churn_potential_updated',\n",
    "        'turbopro_incl_repost_action'\n",
    "    ]\n",
    "    cols_present = [c for c in cols if c in single_agent_df.columns]\n",
    "    agent_df = single_agent_df[cols_present].copy()\n",
    "\n",
    "    # Convert dates\n",
    "    for dt_col in [\"data_month\", \"subscription_start\", \"subscription_end\"]:\n",
    "        if dt_col in agent_df.columns:\n",
    "            agent_df[dt_col] = pd.to_datetime(agent_df[dt_col], errors=\"coerce\")\n",
    "\n",
    "    # Fill numeric columns\n",
    "    numeric_cols = agent_df.select_dtypes(include=[\"number\"]).columns\n",
    "    agent_df[numeric_cols] = agent_df[numeric_cols].fillna(0)\n",
    "\n",
    "    # Agent details\n",
    "    details_cols = [\n",
    "        'agent_id', 'firstname', 'lastname',\n",
    "        'subscription_start', 'subscription_end', 'current_subscription',\n",
    "        'credit_entitlement', 'credit_accumulation_cap', 'credit_purchase_cap',\n",
    "        'listing_segment', 'archetype', 'active_days'\n",
    "    ]\n",
    "    details_cols = [c for c in details_cols if c in agent_df.columns]\n",
    "    agent_details = agent_df[details_cols].drop_duplicates().to_dict(orient=\"records\")[0]\n",
    "\n",
    "    # Archetype matching\n",
    "    archetype_name = agent_details.get(\"archetype\")\n",
    "    agent_archetype_json = next(\n",
    "        (a for a in archetypes if a[\"archetype\"] == archetype_name),\n",
    "        {}\n",
    "    )\n",
    "    agent_archetype_json_str = json.dumps(agent_archetype_json, indent=2)\n",
    "\n",
    "    # Calculate aggregates\n",
    "    tx_cols = [\n",
    "        'landed_sale_transactions', 'hdb_sale_transactions', 'condo_sale_transactions',\n",
    "        'landed_rent_transactions', 'hdb_rent_transactions', 'condo_rent_transactions'\n",
    "    ]\n",
    "    agent_df[\"total_transactions\"] = agent_df[[c for c in tx_cols if c in agent_df.columns]].sum(axis=1, skipna=True)\n",
    "\n",
    "    credit_cols = ['sub_ad_credit_consumed', 'disc_ad_credit_consumed']\n",
    "    agent_df[\"total_credit_consumption\"] = agent_df[[c for c in credit_cols if c in agent_df.columns]].sum(axis=1, skipna=True)\n",
    "\n",
    "    # Monthly data\n",
    "    monthly_cols = [\n",
    "        'data_month', 'total_listings', 'total_credit_consumption',\n",
    "        'total_transactions', 'leads', 'impressions',\n",
    "        'boost_incl_repost_action', 'turbo_incl_repost_action',\n",
    "        'turbopro_incl_repost_action'\n",
    "    ]\n",
    "    monthly_cols = [c for c in monthly_cols if c in agent_df.columns]\n",
    "    agent_df_monthly = agent_df[monthly_cols].copy().sort_values(\"data_month\")\n",
    "    agent_df_monthly[\"month_index\"] = range(len(agent_df_monthly))\n",
    "\n",
    "    # Ensure numeric\n",
    "    for c in ['total_listings', 'total_credit_consumption', 'total_transactions',\n",
    "              'leads', 'impressions', 'boost_incl_repost_action',\n",
    "              'turbo_incl_repost_action', 'turbopro_incl_repost_action']:\n",
    "        if c in agent_df_monthly.columns:\n",
    "            agent_df_monthly[c] = pd.to_numeric(agent_df_monthly[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # Compute trends\n",
    "    total_credit_consumption_slope, total_credit_consumption_trend = compute_trend(agent_df_monthly, \"total_credit_consumption\")\n",
    "    total_transactions_slope, total_transactions_trend = compute_trend(agent_df_monthly, \"total_transactions\")\n",
    "    leads_slope, leads_trend = compute_trend(agent_df_monthly, \"leads\")\n",
    "    impressions_slope, impressions_trend = compute_trend(agent_df_monthly, \"impressions\")\n",
    "    total_listings_slope, total_listings_trend = compute_trend(agent_df_monthly, \"total_listings\")\n",
    "    boost_incl_repost_action_slope, boost_incl_repost_action_trend = compute_trend(agent_df_monthly, \"boost_incl_repost_action\")\n",
    "    turbo_incl_repost_action_slope, turbo_incl_repost_action_trend = compute_trend(agent_df_monthly, \"turbo_incl_repost_action\")\n",
    "\n",
    "    # L365 data\n",
    "    l365_cols = [\n",
    "        'l365_total_listings', 'l365_ad_credit_consumed',\n",
    "        'projected_tier_l365_ad_credit_consumed',\n",
    "        'projected_tier_l365_com_listings',\n",
    "        'projected_tier_l365_newlaunch_listings', 'l365_spend',\n",
    "        'projected_tier_l365_spend_earnings_decile',\n",
    "        'projected_subscription_l365', 'projected_status_l365',\n",
    "        'l365_ad_credit_consumed_tier_decile', 'l365_spend_tier_decile',\n",
    "        'projected_early_upgrade_potential_l365',\n",
    "        'l30_total_listings', 'projected_churn_potential_updated'\n",
    "    ]\n",
    "    l365_cols = [c for c in l365_cols if c in agent_df.columns]\n",
    "    agent_l365 = agent_df[l365_cols].drop_duplicates().to_dict(orient=\"records\")[0] if l365_cols else {}\n",
    "    \n",
    "    # # *** NEW addtiona by RDK: Extract renewal status for the prompt *** if code fails remover\n",
    "    # projected_renewal_status = agent_l365.get(\"projected_status_l365\", \"Unknown\")\n",
    "\n",
    "    # Serialize to JSON\n",
    "    agent_details_json = serialize_for_json(agent_details)\n",
    "    agent_l365_json = serialize_for_json(agent_l365)\n",
    "    \n",
    "    # Extract only relevant field definitions (OPTIMIZATION)\n",
    "    relevant_data_dict_str = extract_relevant_fields(data_dict, agent_df)\n",
    "\n",
    "    agent_details_str = json.dumps(agent_details_json, indent=2)\n",
    "    agent_l365_str = json.dumps(agent_l365_json, indent=2)\n",
    "\n",
    "    # Build optimized prompt\n",
    "    prompt = f\"\"\"\n",
    "====================\n",
    "System\n",
    "====================\n",
    "Act like a veteran Sales Manager and Data Analyst specializing in agent performance and health analytics for Singapore property marketplace listings.\n",
    "You are coaching a Sales Person through an evidence-based review of a single agent's performance.\n",
    "Your job: Interpret the agent's behavior, assess their overall account health (growth, stability, or decline), and propose targeted engagement actions using ONLY the provided data dictionary and agent datasets.\n",
    "\n",
    "====================\n",
    "Inputs\n",
    "====================\n",
    "1. Data Dictionary (Relevant Fields Only): {relevant_data_dict_str}\n",
    "2. Agent details: {agent_details_str}\n",
    "3. Agent data for last 365 days: {agent_l365_str}\n",
    "4. Archetype Profile: {agent_archetype_json_str}\n",
    "\n",
    "Archetype Components:\n",
    "- Pain Points: Main challenges the agent experiences. Use this to interpret trends and understand barriers to engagement.\n",
    "- What They Value: Motivations and priorities. Use this to frame recommendations that align with their goals.\n",
    "- PG Messaging Principles: Best communication approaches. Use this to craft interventions aligned with their style.\n",
    "\n",
    "====================\n",
    "Objective\n",
    "====================\n",
    "1. Describe what's happening with this agent based on data trends and context\n",
    "2. Assess overall account health: Is the agent growing, stable, at-risk, or churning?\n",
    "3. Deliver concrete, prioritized actions the sales manager can execute now\n",
    "4. Hard limit: Section 1 ‚â§250 words, Section 2 ‚â§150 words\n",
    "\n",
    "====================\n",
    "Guardrails\n",
    "====================\n",
    "1. DO NOT start the output with any conversational text, preamble, or introduction like \"Alright team...\" or \"Here's a review...\". The output must begin directly with the \"1. Summary...\" header.\n",
    "2. Do not fabricate metrics not derivable from the inputs.\n",
    "3. Reference fields exactly as named in the data dictionary.\n",
    "4. Never infer peer benchmarks unless provided via \"tier_decile\" fields. Top 40% = 40th percentile within tier.\n",
    "5. Ignore fields in data dictionary that are not in datasets.\n",
    "6. Agents in Bronze, Silver, Silver Plus, Advance, and Standard tiers CANNOT use \"Turbo Pro\" actions. Do not evaluate them on this.\n",
    "7. Describe their last-365-days ad credit consumption decile vs peers in their tier (do not print the field name).\n",
    "8. Never print slope values. Only state: upward trend / downward trend / consistent / no trend.\n",
    "\n",
    "====================\n",
    "KEY HEALTH SIGNALS TO ASSESS\n",
    "====================\n",
    "I provide trend analysis variables from regression slopes across 7 agent activity metrics over 12 months.\n",
    "\n",
    "**Trend Interpretation:**\n",
    "- **Upward trend**: Strong engagement, growth opportunity, potential for upsell\n",
    "- **Downward trend**: Declining engagement, churn risk, needs intervention\n",
    "- **Consistent**: Stable performance, maintain relationship\n",
    "- **No trend**: Insufficient data or inactive\n",
    "\n",
    "=== Metric Definitions ===\n",
    "total_credit_consumption: Advertising spend. Upward = increasing investment. Downward = disengagement or budget cuts.\n",
    "total_transactions: Closed deals. Upward = business success. Downward = struggling pipeline.\n",
    "leads: Inquiry volume. Upward = strong listing performance. Downward = weakening visibility.\n",
    "impressions: Platform visibility. Upward = active promotion. Downward = reduced activity.\n",
    "total_listings: Inventory count. Upward = expanding business. Downward = shrinking portfolio.\n",
    "boost_incl_repost_action: Basic promotion usage. Upward = increasing effort. Downward = reduced belief in tools.\n",
    "turbo_incl_repost_action: Premium promotion usage. Upward = premium engagement. Downward = cost-cutting.\n",
    "\n",
    "=== Trend Variables ===\n",
    "total_credit_consumption: {total_credit_consumption_trend}\n",
    "total_transactions: {total_transactions_trend}\n",
    "leads: {leads_trend}\n",
    "impressions: {impressions_trend}\n",
    "total_listings: {total_listings_trend}\n",
    "boost_incl_repost_action: {boost_incl_repost_action_trend}\n",
    "turbo_incl_repost_action: {turbo_incl_repost_action_trend}\n",
    "\n",
    "=== Analysis Instructions ===\n",
    "1. Identify upward, downward, consistent, and no-trend metrics.\n",
    "2. Assess overall health: Growing (multiple upward trends) / Stable (consistent) / At-Risk (some downward) / Churning (multiple downward).\n",
    "3. For new agents (active_days < 270): Note limited data and lower confidence in trend assessment.\n",
    "4. Context matters: Interpret trends through archetype pain points and values.\n",
    "5. If no specific growth or recovery actions are needed (e.g., for a stable agent), recommend a simple relationship-building check-in.\n",
    "\n",
    "\n",
    "====================\n",
    "OUTPUT SPECIFICATION\n",
    "====================\n",
    "Output EXACTLY two sections with the headers \"1. Summary of Agent Health & Behaviour**\" and \"2. Recommended Engagement Actions\". DO NOT omit the second section for any reason.\n",
    "\n",
    "**1. Summary of Agent Health & Behaviour**\n",
    "- State: firstname, lastname, archetype, listing segment, current subscription tier\n",
    "- Describe their performance vs tier peers using l365_ad_credit_consumed_tier_decile\n",
    "- Summarize trend patterns: Which metrics are growing, stable, or declining?\n",
    "- Assess overall health status: Growing / Stable / At-Risk / Churning\n",
    "- Interpret trends in context of their archetype pain points and values\n",
    "- Do NOT print slope numbers or field names\n",
    "\n",
    "**2. Recommended Engagement Actions [Beta]**\n",
    "- Provide 2-3 concrete, prioritized actions the sales manager can execute now\n",
    "- For growing agents: Upsell opportunities, recognition, expansion strategies\n",
    "- For stable agents: Retention tactics, loyalty rewards, efficiency tools\n",
    "- For at-risk agents: Re-engagement, support, win-back incentives\n",
    "- For churning agents: Urgent intervention, root cause discovery, rescue offers\n",
    "- Include 1 sentence that mirrors PG messaging principle phrasing\n",
    "- Tailor to agent's ambition level (breaking through / sustaining / achieving icon status)\n",
    "\n",
    "====================\n",
    "VALIDATION CHECKLIST (DO NOT OUTPUT)\n",
    "====================\n",
    "‚úì Used data dictionary to define every metric?\n",
    "‚úì Avoided printing column names?\n",
    "‚úì Avoided penalizing Turbo Pro nulls for Bronze/Silver/Silver Plus/Advance/Standard?\n",
    "‚úì Output limited to two sections only?\n",
    "‚úì No fabricated metrics?\n",
    "‚úì Assessed overall health, not just churn?\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt, agent_details\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b459ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# Single Agent Processor with Error Handling\n",
    "# =========================================================\n",
    "def process_single_agent(agent_id, df_agent):\n",
    "    \"\"\"Process one agent with retry logic and error handling\"\"\"\n",
    "    try:\n",
    "        prompt, details = build_prompt_for_single_agent(df_agent)\n",
    "        prompt_tokens = count_tokens(prompt)\n",
    "        \n",
    "        print(f\" Agent {agent_id} | tokens={prompt_tokens:,}\")\n",
    "        \n",
    "        llm_output = query_gemini(\n",
    "            prompt,\n",
    "            max_output_tokens=10248,\n",
    "            timeout=90\n",
    "        )\n",
    "        \n",
    "        if not llm_output:\n",
    "            print(f\"Empty response for agent {agent_id}\")\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            \"agent_id\": agent_id,\n",
    "            \"firstname\": details.get(\"firstname\"),\n",
    "            \"lastname\": details.get(\"lastname\"),\n",
    "            \"archetype\": details.get(\"archetype\"),\n",
    "            \"current_subscription\": details.get(\"current_subscription\"),\n",
    "            \"listing_segment\": details.get(\"listing_segment\"),\n",
    "            \"active_days\": details.get(\"active_days\"),\n",
    "            \"summary\": llm_output,\n",
    "            \"prompt_tokens\": prompt_tokens\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Agent {agent_id}: {str(e)}\")\n",
    "        return {\n",
    "            \"agent_id\": agent_id,\n",
    "            \"error\": str(e),\n",
    "            \"firstname\": None,\n",
    "            \"lastname\": None,\n",
    "            \"summary\": None\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# Run Parallel Processing with Progress Tracking\n",
    "# =========================================================\n",
    "print(f\"\\n Starting parallel processing with {MAX_WORKERS} workers...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {\n",
    "        executor.submit(process_single_agent, agent_id, df_agent): agent_id\n",
    "        for agent_id, df_agent in agent_df_all.groupby(\"agent_id\")\n",
    "    }\n",
    "    \n",
    "    # Progress bar\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing agents\"):\n",
    "        res = future.result()\n",
    "        if res:\n",
    "            results.append(res)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Separate successful vs failed\n",
    "successful_df = results_df[results_df['summary'].notna()].copy()\n",
    "failed_df = results_df[results_df['summary'].isna()].copy()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "# print(f\"‚úÖ Successful: {len(successful_df)}/{len(results_df)}\")\n",
    "# print(f\"‚ùå Failed: {len(failed_df)}\")\n",
    "print(f\"Total time: {elapsed:.1f}s (~{elapsed/60:.2f} minutes)\")\n",
    "print(f\"‚ö° Avg time per agent: {elapsed/len(results_df):.2f}s\")\n",
    "\n",
    "if len(successful_df) > 0:\n",
    "    avg_tokens = successful_df['prompt_tokens'].mean()\n",
    "    total_tokens = successful_df['prompt_tokens'].sum()\n",
    "    print(f\"Avg prompt tokens: {avg_tokens:,.0f}\")\n",
    "    print(f\"Total tokens: {total_tokens:,.0f}\")\n",
    "    print(f\"Est. cost (Flash): ${total_tokens * 0.000001:.4f}\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433cc46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(successful_df) > 0:\n",
    "    avg_tokens = successful_df['prompt_tokens'].mean()\n",
    "    total_tokens = successful_df['prompt_tokens'].sum()\n",
    "    print(f\"üé´ Avg prompt tokens: {avg_tokens:,.0f}\")\n",
    "    print(f\"üé´ Total tokens: {total_tokens:,.0f}\")\n",
    "    print(f\"üí∞ Est. cost (Flash): ${total_tokens * 0.000001:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b17fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7585d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in results_df.iterrows():\n",
    "    print(f\"Agent: {row['firstname']} {row['lastname']}\")\n",
    "    print(row['summary'])\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305920a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Analyze Results Distribution\n",
    "# =========================================================\n",
    "if len(successful_df) > 0:\n",
    "    print(\"\\nüìä RESULTS ANALYSIS\\n\")\n",
    "    \n",
    "    print(\"By Archetype:\")\n",
    "    print(successful_df['archetype'].value_counts())\n",
    "    \n",
    "    print(\"\\nBy Subscription Tier:\")\n",
    "    print(successful_df['current_subscription'].value_counts())\n",
    "    \n",
    "    print(\"\\nBy Listing Segment:\")\n",
    "    print(successful_df['listing_segment'].value_counts())\n",
    "    \n",
    "    # Token usage distribution\n",
    "    print(\"\\nToken Usage Statistics:\")\n",
    "    print(successful_df['prompt_tokens'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the save directory\n",
    "save_dir = \"/Users/rohankajgaonkar/Downloads\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "if len(successful_df) > 0:\n",
    "    output_file = os.path.join(save_dir, f\"agent_health_summaries_{timestamp}.csv\")\n",
    "    successful_df.to_csv(output_file, index=False)\n",
    "    print(f\"üíæ Saved successful results to: {output_file}\")\n",
    "\n",
    "if len(failed_df) > 0:\n",
    "    error_file = os.path.join(save_dir, f\"failed_agents_{timestamp}.csv\")\n",
    "    failed_df.to_csv(error_file, index=False)\n",
    "    print(f\"‚ö†Ô∏è  Saved failed agents to: {error_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
